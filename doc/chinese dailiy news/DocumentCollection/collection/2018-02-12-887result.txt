Google相簿(Google Photos)在2015年曾鬧出將黑人女性自動標籤為「Gorillas」(大猩猩)的事件，引發軒然大波，還被指出有種族歧視意味，對此，Google不僅急速修改演算法，更出面道歉。無獨有偶的是，近日有論文以商用人臉辨識技術為研究目標，結果發現人臉辨識技術對黑人女性的辨識錯誤率最高，彷彿是Google Photos烏龍事件的重演。在蘋果推出iPhone X之後，人臉辨識技術一時成為顯學，市場討論度提升不少。為了讓讀者更為了解相關技術細節，《紐約時報》報導了MIT媒體實驗室(MIT Media Lab)研究員Joy Buolamwini與微軟科學家Timnit Gebru共同合作的《Gender Shades: Intersectional Accuracy Disparitiesin Commercia lGender Classification》論文內容。論文中使用了微軟、IBM以及曠視(中國大陸一家專注於人臉辨識的新創公司)的人臉辨識API(應用程式介面)，來了解這些技術在性別判定的準確性。論文結果發現，以上三家廠商的商用人臉辨識技術中，對於黑人女性的辨識錯誤率普遍都很高，高於白人男性。論文利用了自行蒐集的一組Pilot Parliaments Benchmark(PPB)資料庫進行測試，包含1270張人臉，分別來自三個非洲國家與三個歐洲國家。論文作者在進行研究後，有以下幾個發現：1.所有分類器在辨識男性人臉的表現優於女性人臉(8.1%、20.6%的錯誤率差別)2.所有分類器在膚色較白的人臉表現上，優於膚色較深的人臉(11.8%、19.2%的錯誤率差別)3.所有分類器在膚色較深的女性人臉上表現最差(錯誤率在20.8%~34.7%之間)4.微軟與IBM的分類器，在淺膚色男性人臉的表現最好(錯誤率分別是0%以及0.3%)。而曠視的分類器在膚色較深的男性人臉上表現最好(錯誤率0.7%)報導指出，論文作者之一的Joy Buolamwini乃是黑人女性，他在進行研究之前曾發現人臉辨識工具無法辨識她的臉，只有在她戴上一張白色面具才可行。這讓她有了研究此議題的動力。從論文的研究可以發現，確實作者有嘗試探討AI(人工智慧)技術是否存在種族歧視的情況。針對以上發現，各家廠商都進行了回應。結果發現，促成以上結果的主因有二，第一是深色人種資料庫的缺乏，以及與深色人種的人臉特徵較難偵測有關。因此，雖然該研究發現人臉辨識工具普遍在辨識膚色較深的女性時錯誤率較高，但絕不能太快推論當中存有種族歧視的意圖。而由此也能發現，訓練AI相關模型時，調教出來的工具是否辨識度高、適用性廣，跟當初用來訓練AI的資料庫有著密不可分的關係(中時電子報)<span>文章來源：</span><a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html"
        target="_blank" rel="nofollow">Facial Recognition Is Accurate, if You’re a White Guy</a>