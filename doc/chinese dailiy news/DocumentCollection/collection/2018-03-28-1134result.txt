NVIDIA宣布一連串新技術與合作夥伴，將推論的潛在市場拓展至全球3,000萬部超大規模伺服器，同時大幅降低由深度學習技術所驅動的各種服務成本。NVIDIA創辦人暨執行長黃仁勳在2018 GTC發表開幕演說時，闡述深度學習推論如何透過在資料中心、車用以及如機器人和無人機等嵌入式裝置上，新增語音辨識、自然語言處理、推薦系統與影像辨識等技術支援，持續仰賴繪圖處理器（GPU）加速。NVIDIA宣布新版TensorRT推論軟體並將其整合至Google熱門的TensorFlow框架中，NVIDIA也宣布將針對最受歡迎的語音辨識框架Kaldi進行GPU最佳化。此外，NVIDIA與Amazon、Facebook以及Microsoft等夥伴的密切合作，也將讓開發人員更容易將ONNX格式與WinML模型透過GPU進行加速。NVIDIA揭露TensorRT 4軟體能為多種應用程式進行深度學習推論加速。TensorRT能提供INT8與FP16精準的推論內容，讓資料中心成本最高可減少70％。在處理包括電腦視覺、神經機器翻譯、自動語音辨識、語音合成與推薦系統等常見應用時，新版軟體在執行深度學習推論的速度是CPU的190倍。NVIDIA工程師與包括Amazon、Facebook以及Microsoft等公司密切合作，確保運用各種ONNX框架的開發人員，包括Caffe 2、Chainer、CNTK、MXNet和Pytorch的用戶，現在都能輕易部署至NVIDIA的深度學習平台上。SAP機器學習部門經理Markus Noga表示，針對TensorRT基於深度學習在NVIDIA Tesla V100 GPU上運行SAP所推薦的應用程式之評測，推論速度與吞吐量比CPU平台快45倍。透過NVIDIA Tesla GPU加速的伺服器能取代數個機架的CPU伺服器，空出寶貴的機架空間並降低對能源與冷卻的需求。TensorRT也能部署在NVIDIA DRIVE自駕車與NVIDIA Jetson嵌入式平台。每個框架上的深度學習神經網路都能在資料中心的NVIDIA DGX系統上進行訓練，並部署從機器人到自駕車所有種類的裝置上，在邊緣進行即時推論。透過TensorRT，開發人員能專注於研發新穎的深度學習應用，而不用費心為推論部署進行繁瑣的效能調校。(工商 )